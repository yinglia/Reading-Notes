Wiki  https://en.wikipedia.org/wiki/Adversarial_machine_learning

Other sources: 

https://towardsdatascience.com/know-your-enemy-the-fascinating-implications-of-adversarial-examples-5936bccb24af

https://www.toptal.com/machine-learning/adversarial-machine-learning-tutorial

https://www.kdnuggets.com/2019/06/machine-learning-adversarial-attacks.html

Nowadays, as machine learning models are deployed in real-world scenarios, security vulnerabilities coming from model errors have become a real concern.

Goal: explain and demonstrate how state-of-the-art deep neural networks used in image recognition can be easily fooled by a malicious actor and thus made to produce wrong predictions. Once we become familiar with the usual attack strategies, we will discuss how to defend our models against them.

define:

Adversarial examples are malicious inputs purposely designed to fool a machine learning model.

etc.  input images crafted by an attacker that the model is not able to classify correctly

attacker add noise to a panda image dilebaretly and human eye can still recognize the image as pandas, but the machine learning will recognize it as a gibon even with higher conficence.

adversarial example for humman:

![image](https://user-images.githubusercontent.com/90790297/160697394-f663f83c-106b-4acb-bc7c-4e94f28684a1.png)


Adversarial examples are generated by taking a clean image that the model correctly classifies, and finding a small perturbation that causes the new image to be misclassified by the ML model.

Let’s suppose that an attacker has complete information about the model they want to attack. This essentially means that the attacker can compute the loss function of the model 
J(θ,X,y) where X is the input image, y is the output class, and θ are the internal model parameters. This loss function is typically the negative loss likelihood for classification methods.

Under this white-box scenario, there are several attacking strategies, each of them representing different tradeoffs between computational cost to produce them and their success rate. All these methods essentially try to maximize the change in the model loss function while keeping the perturbation of the input image small. The higher the dimension of the input image space is, the easier it is to generate adversarial examples that are indistinguishable from clean images by the human eye.

## L-BFGS Method

## FAST GRADIENT SIGN (FGS)

## ITERATIVE FAST GRADIENT SIGN

## BLACK-BOX ATTACK


# How to defense

Obviously, the less information the model outputs at prediction time, the harder it is for an attacker to craft a successful attack.

When confidence scores are provided to the end user, a malicious attacker can use them to numerically estimate the gradient of the loss function. This way, attackers can craft white-box attacks using, for example, fast gradient sign method. In the Computer Vision Foundation paper we quoted earlier, the authors show how to do this against a commercial machine learning model.

# Part 2

https://towardsdatascience.com/know-your-enemy-the-fascinating-implications-of-adversarial-examples-5936bccb24af

## Why are neural networks vulnerable to these attacks?

Even though this is possibly the most interesting question surrounding adversarial examples I can’t give you a satisfying answer — this is a completely open question! Many proposals have been put forward including
Neural networks are too linear in regions of the input space (ref)
Dependence of prediction on weakly correlating features (ref)
Large singular values of internal weight matrices (ref)
Adversarial examples are a perhaps unavoidable property of high dimensional input spaces (ref)
